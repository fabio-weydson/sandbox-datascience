{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "144.545px",
        "left": "938.091px",
        "right": "20px",
        "top": "120px",
        "width": "350px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": true
    },
    "colab": {
      "name": "Cópia de CE_1.4_por.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabio-weydson/sandbox-datascience/blob/main/CE_1_4_Agrupamento%20de%20Noticias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXMU9GKSXOA_"
      },
      "source": [
        "# Estudo de caso 1.4: Agrupamento espectral - Agrupamento de notícias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcQISTo9XOBA"
      },
      "source": [
        "---\n",
        "<br>\n",
        "\n",
        "Este estudo de caso considera um banco de dados de artigos da imprensa, sobre diferentes temas, e usa _agrupamento espectral_ para agrupá-los de acordo com a frequência de determinadas palavras. Este notebook fornece o código para gerar o banco de dados.\n",
        "\n",
        "Este estudo de caso usa a biblioteca [`mitie`](https://github.com/mit-nlp/MITIE), desenvolvida no MIT. Todas as etapas para instalar a biblioteca e o modelo NER usados ​​neste estudo de caso podem ser encontradas na documentação online.\n",
        "\n",
        "<br>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh7s2KbgXOBB"
      },
      "source": [
        "Configuração do notebook:\n",
        "\n",
        "* Primeiramente, baixe a biblioteca MITIE a partir do seu repositório de GitHub, instale-a no ambiente de execução e baixe seus principais modelos de *NLP*, dentre eles o modelo `NER` que usaremos neste estudo de caso.\n",
        "\n",
        "* Depois, instale o restante das bibliotecas necessárias e o modelo `NER` em uma variável de forma que possamos usá-lo no estudo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoPM3Rs3GMf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3009803b-5d71-4d21-e5a0-1babff633c3e"
      },
      "source": [
        "!pip3 install git+https://github.com/mit-nlp/MITIE.git\n",
        "!wget https://github.com/mit-nlp/MITIE/releases/download/v0.4/MITIE-models-v0.2.tar.bz2\n",
        "!tar jxf MITIE-models-v0.2.tar.bz2\n",
        "\n",
        "print('MITIE instalado com sucesso e modelos baixados!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/mit-nlp/MITIE.git\n",
            "  Cloning https://github.com/mit-nlp/MITIE.git to /tmp/pip-req-build-juquf_qo\n",
            "  Running command git clone -q https://github.com/mit-nlp/MITIE.git /tmp/pip-req-build-juquf_qo\n",
            "Building wheels for collected packages: mitie\n",
            "  Building wheel for mitie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mitie: filename=mitie-0.7.0-cp37-none-any.whl size=418688 sha256=b3e37f7e00af5cf039de01fc74d67258d7a4b871904f0da871e6787e7df7dce1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s5czsaz7/wheels/b4/c1/21/8e7e7e14cf3211bf5c73aad0b1d76d1186fbf681f4b9ef6c06\n",
            "Successfully built mitie\n",
            "Installing collected packages: mitie\n",
            "Successfully installed mitie-0.7.0\n",
            "--2021-06-01 10:45:51--  https://github.com/mit-nlp/MITIE/releases/download/v0.4/MITIE-models-v0.2.tar.bz2\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/18347668/ac81c21e-faa5-11e5-9bb1-a2b9688a3fbc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210601%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210601T104551Z&X-Amz-Expires=300&X-Amz-Signature=415f93a44ef8872b098cfb07f3c41b12a58c3e3f22697c439281948d7e854caf&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=18347668&response-content-disposition=attachment%3B%20filename%3DMITIE-models-v0.2.tar.bz2&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-06-01 10:45:51--  https://github-releases.githubusercontent.com/18347668/ac81c21e-faa5-11e5-9bb1-a2b9688a3fbc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210601%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210601T104551Z&X-Amz-Expires=300&X-Amz-Signature=415f93a44ef8872b098cfb07f3c41b12a58c3e3f22697c439281948d7e854caf&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=18347668&response-content-disposition=attachment%3B%20filename%3DMITIE-models-v0.2.tar.bz2&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.111.154, 185.199.108.154, 185.199.109.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.111.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 435617407 (415M) [application/octet-stream]\n",
            "Saving to: ‘MITIE-models-v0.2.tar.bz2’\n",
            "\n",
            "MITIE-models-v0.2.t 100%[===================>] 415.44M  79.5MB/s    in 5.6s    \n",
            "\n",
            "2021-06-01 10:45:57 (74.4 MB/s) - ‘MITIE-models-v0.2.tar.bz2’ saved [435617407/435617407]\n",
            "\n",
            "MITIE instalado com sucesso e modelos baixados!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T17:10:34.209671Z",
          "start_time": "2020-06-10T17:10:30.066790Z"
        },
        "init_cell": true,
        "id": "gfqmwsQNXOBC"
      },
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import csv\n",
        "\n",
        "#ML\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import cluster\n",
        "\n",
        "#Bibliotecas de web scraping\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#NLP\n",
        "from mitie import *\n",
        "print('Bibliotecas importadas com sucesso!\\n')\n",
        "print(\"Carregando o modelo NER...\")\n",
        "ner = named_entity_extractor('MITIE-models/english/ner_model.dat')\n",
        "print(\"\\nEtiquetas de saída do modelo NER:\", ner.get_possible_ner_tags())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFyc0j2zXOBI"
      },
      "source": [
        "# Geração do banco de dados (Web Scraping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW8JUnI0XOBI"
      },
      "source": [
        "Neste exemplo, foram compilados artigos de 8 temas diferentes do jornal britânico __The Guardian__. A seguir, temos as etapas para criar o banco de dados:\n",
        "\n",
        "1. Obter o código fonte do site principal do The Guardian e armazenar os links das seções (temas) de interesse.\n",
        "2. Iterar a lista de links e obter a informação de 10 artigos por seção (título e conteúdo).\n",
        "3. Salvar os artigos, títulos e temas em arquivos `.txt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-08T23:14:11.530709Z",
          "start_time": "2020-06-08T23:14:10.866851Z"
        },
        "id": "pdjoGkujXOBJ"
      },
      "source": [
        "UK_news_url = 'https://www.theguardian.com/uk'\n",
        "#Baixando os links dos diferentes temas\n",
        "html_data = requests.get(UK_news_url).text\n",
        "soup = BeautifulSoup(html_data, 'html.parser')\n",
        "url_topics = [el.find('a')['href'] for el in soup.find_all(class_ = 'subnav__item')[1:9]]\n",
        "topics = [el.text.strip('\\n').replace(' ','_') for el in soup.find_all(class_ = 'subnav-link')[1:9]]\n",
        "for i in range(len(topics)):\n",
        "    print('Topic {}: {} ({})'.format(i+1,topics[i],url_topics[i]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-08T23:14:54.791725Z",
          "start_time": "2020-06-08T23:14:11.533015Z"
        },
        "id": "McLdTmh1XOBO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "9a2323d9-fa7c-49c0-aead-95fae1d3a666"
      },
      "source": [
        "def save_to_txt(filename, content):\n",
        "    '''\n",
        "    Creates a new .txt file with as specific name in the Data directory\n",
        "    '''\n",
        "    with open(r\"Data/{}.txt\".format(filename), \"w\") as f:\n",
        "        print(content, file=f)\n",
        "\n",
        "#Cria-se um diretório onde serão salvos os artigos\n",
        "os.mkdir('Data/')\n",
        "\n",
        "\n",
        "article_titles = []\n",
        "article_contents = []\n",
        "article_topics = []\n",
        "articles_per_topic = 10\n",
        "n = 1\n",
        "for topic, url_topic in list(zip(topics,url_topics)):\n",
        "    #Getting the first 15\n",
        "    soup = BeautifulSoup(requests.get(url_topic).text, 'html.parser')\n",
        "    url_articles = [el.find('a')['href'] for el in soup.find_all(class_ = 'fc-item__content')]\n",
        "    print('\\n{}:'.format(topic))\n",
        "    i = 0\n",
        "    while article_topics.count(topic) < articles_per_topic:\n",
        "        soup = BeautifulSoup(requests.get(url_articles[i]).text, 'html.parser')\n",
        "        try:\n",
        "            title = soup.find(class_ = 'content__headline').text.strip('\\n')\n",
        "            content = ' '.join([el.text for el in soup.find(class_ = 'content__article-body from-content-api js-article__body').find_all('p')])\n",
        "            i += 1\n",
        "            if i == len(url_articles):\n",
        "                print('Only {} articles found in \\\"{}\"'.format(article_topics.count(topic),topic))\n",
        "                break\n",
        "            if title not in article_titles:\n",
        "                article_titles += [title]\n",
        "                article_contents += [content]\n",
        "                article_topics += [topic]\n",
        "                save_to_txt('title-{}'.format(n),title)\n",
        "                save_to_txt('article-{}'.format(n),content)\n",
        "                save_to_txt('topic-{}'.format(n),topic)\n",
        "                print('{}'.format(title))\n",
        "                n += 1\n",
        "                if round(len(article_titles)/10) == len(article_titles)/10:\n",
        "                    print('Article count: {}'.format(len(article_titles)))\n",
        "        except:\n",
        "            i += 1\n",
        "            if i == len(url_articles):\n",
        "                print('Only {} articles found in \\\"{}\"'.format(article_topics.count(topic),topic))\n",
        "                break\n",
        "            pass\n",
        "        \n",
        "                \n",
        "df = pd.DataFrame({'topic':article_topics,'title':article_titles,'content':article_contents})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2b6a96d7e7ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Cria-se um diretório onde serão salvos os artigos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3_qHcBpXOBT"
      },
      "source": [
        "# Importação do banco de dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkqzHyhBXOBU"
      },
      "source": [
        "Após salvar o banco de dados na pasta desejada, podemos usar o código do estudo de caso para importar a informação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-08T23:14:54.827485Z",
          "start_time": "2020-06-08T23:14:54.793892Z"
        },
        "id": "KLKAiyUIXOBV"
      },
      "source": [
        "#número total de artigos a serem processados\n",
        "N = df.shape[0]\n",
        "#para armazenar os temas, títulos e conteúdos das notícias:\n",
        "topics_array = []\n",
        "titles_array = []\n",
        "corpus = []\n",
        "for i in range(1, N+1):\n",
        "    #obtenha o conteúdo do artigo.\n",
        "    with open('Data/article-' + str(i) + '.txt', 'r') as myfile:\n",
        "        d1=myfile.read().replace('\\n', '')\n",
        "        d1 = d1.lower()\n",
        "        corpus.append(d1)\n",
        "    #obtenha o tema original do artigo.\n",
        "    with open('Data/topic-' + str(i) + '.txt', 'r') as myfile:\n",
        "        to1=myfile.read().replace('\\n', '')\n",
        "        to1 = to1.lower()\n",
        "        topics_array.append(to1)\n",
        "    #obtenha o título do artigo.\n",
        "    with open('Data/title-' + str(i) + '.txt', 'r') as myfile:\n",
        "        ti1=myfile.read().replace('\\n', '')\n",
        "        ti1 = ti1.lower()\n",
        "        titles_array.append(ti1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ-OVQfTXOBa"
      },
      "source": [
        "# Geração de atributos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SinUjKUDXOBb"
      },
      "source": [
        "Para gerar os atributos de cada instância (artigo):\n",
        "\n",
        "1. Vinculamos todo o corpus de texto do artigo para determinar todas as palavras únicas que são usadas no conjunto de dados.\n",
        "2. Procuramos o subconjunto das entidades do modelo NER encontrado entre as palavras únicas que são usadas no conjunto de dados (determinado na etapa 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-08T23:33:56.085401Z",
          "start_time": "2020-06-08T23:33:54.892765Z"
        },
        "id": "xQKFw-BKXOBb"
      },
      "source": [
        "#vetor de subconjunto de entidades\n",
        "entity_text_array = [] \n",
        "for i in range(1, N+1):\n",
        "    #carregue o arquivo de texto con o conteúdo do artigo e converta-o em uma lista de palavras\n",
        "    tokens = tokenize(load_entire_file(('Data/article-' + str(i) + '.txt')))\n",
        "    #extraia todas as entidades conhecidas do modelo ner mencionado neste artigo\n",
        "    entities = ner.extract_entities(tokens)\n",
        "    #extraia as palavras de entidades reais adicione-as ao vetor\n",
        "    for e in entities: \n",
        "        range_array = e[0]\n",
        "        tag = e[1]\n",
        "        score = e[2]\n",
        "        score_text = \"{:0.3f}\".format(score)\n",
        "        entity_text = \" \".join(tokens[j].decode(\"utf-8\") for j in range_array) \n",
        "        entity_text_array.append(entity_text.lower())\n",
        "#elimine as entidades duplicadas que foram detectadas\n",
        "#entity_text_array = np.unique(entity_text_array)\n",
        "entity_text_array = list(set(entity_text_array))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-22T19:12:49.108073Z",
          "start_time": "2020-05-22T19:12:49.094235Z"
        },
        "id": "0nHjCRawXOBf"
      },
      "source": [
        "Agora que temos a lista de todas as entidades utilizadas no banco de dados, podemos representar cada artigo como um vetor que contém a pontuação de [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf) para cada entidade armazenada no `entity_text_array`. Esta tarefa pode ser realizada facilmente com a biblioteca [scikit-learn](http://scikit-learn.org/stable/) de Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-08T23:14:56.146658Z",
          "start_time": "2020-06-08T23:14:56.098058Z"
        },
        "id": "0X8ZtG-hXOBg"
      },
      "source": [
        "vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word',\n",
        "                       stop_words='english', vocabulary=entity_text_array)\n",
        "corpus_tf_idf = vect.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGlHMSkvXOBk"
      },
      "source": [
        "Agora que temos os artigos representados por seus atributos (pontuações de TF-IDF), podemos fazer o agrupamento espectral deles usando novamente a biblioteca `scikit-learn`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-08T23:14:56.211117Z",
          "start_time": "2020-06-08T23:14:56.148577Z"
        },
        "id": "sp0ILK_gXOBl"
      },
      "source": [
        "#Altere n_clusters para o número de grupos desejados  \n",
        "n_clusters = 8\n",
        "#Agrupamento espectral \n",
        "spectral = cluster.SpectralClustering(n_clusters= n_clusters, \n",
        "                                      eigen_solver='arpack', \n",
        "                                      affinity=\"nearest_neighbors\", \n",
        "                                      n_neighbors = 10)\n",
        "spectral.fit(corpus_tf_idf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia8cI0j4XOBp"
      },
      "source": [
        "Por fim, as linhas de código a seguir permitem ver o resultado no seguinte formato (uma linha por artigo):\n",
        "\n",
        "<br>\n",
        "\n",
        "__no. artigo, tema, grupo, título__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-08T23:14:56.228444Z",
          "start_time": "2020-06-08T23:14:56.215438Z"
        },
        "id": "YLvoJlvsXOBq"
      },
      "source": [
        "if hasattr(spectral, 'labels_'):\n",
        "    cluster_assignments = spectral.labels_.astype(np.int)\n",
        "    for i in range(0, len(cluster_assignments)):\n",
        "        print(i, topics_array[i], cluster_assignments [i], titles_array[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-08T23:32:21.170169Z",
          "start_time": "2020-06-08T23:32:21.154055Z"
        },
        "id": "Ws_Zi7TKXOBu"
      },
      "source": [
        "df['predictions'] = cluster_assignments\n",
        "predictions_df = pd.get_dummies(df, columns=['predictions']).drop(['title','content'],axis=1).groupby(['topic']).sum()\n",
        "predictions_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW1q0B5iXOB1"
      },
      "source": [
        "Como podemos ver, o algoritmo nem sempre classifica os artigos de acordo com as seções de onde foram obtidos. Você pode se aprofundar nos parâmetros do modelo para melhorar esses resultados ou procurar uma explicação para entender os critérios pelos quais o algoritmo está agrupando os artigos."
      ]
    }
  ]
}